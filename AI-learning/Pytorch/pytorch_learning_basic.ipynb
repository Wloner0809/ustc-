{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量(tensor)\n",
    "* 张量创建\n",
    "\n",
    "<img src=\"pic/pic1.png\" width = \"50%\" />\n",
    "\n",
    "* 张量操作\n",
    "* 张量广播机制\n",
    "* 索引和切片\n",
    "  * `0`表示第一个元素的索引, `-1`表示最后一个元素的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor1:\n",
      " tensor([[0.4464, 0.3749, 0.5388],\n",
      "        [0.9854, 0.3688, 0.3479],\n",
      "        [0.9173, 0.3776, 0.4577],\n",
      "        [0.8208, 0.8751, 0.3079]])\n",
      "tensor2:\n",
      " tensor([[-0.1097,  1.3128, -0.3790],\n",
      "        [-0.1051,  0.9571,  0.9689],\n",
      "        [ 0.7867, -0.6162, -0.6264],\n",
      "        [-0.6189, -0.8200, -0.9993]])\n",
      "tensor3:\n",
      " tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "tensor4:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor5:\n",
      " tensor([1, 2, 3])\n",
      "tensor6:\n",
      " tensor([0, 2, 4, 6, 8])\n",
      "tensor7:\n",
      " tensor([ 0.,  2.,  4.,  6.,  8., 10.]) torch.Size([6]) torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(4, 3)\n",
    "print(\"tensor1:\\n\", x)\n",
    "x = torch.randn(4, 3)\n",
    "print(\"tensor2:\\n\", x)\n",
    "x = torch.zeros(4, 3, dtype=torch.long)\n",
    "print(\"tensor3:\\n\", x)\n",
    "x = torch.ones(4, 3, dtype=torch.float32)\n",
    "print(\"tensor4:\\n\", x)\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(\"tensor5:\\n\", x)\n",
    "x = torch.arange(0, 10, 2)\n",
    "print(\"tensor6:\\n\", x)\n",
    "x = torch.linspace(0, 10, 6)\n",
    "print(\"tensor7:\\n\", x, x.shape, x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x, y:\n",
      " tensor([[0.1218, 0.9291, 0.9322],\n",
      "        [0.8839, 0.8989, 0.9763]]) \n",
      " tensor([[0.9876, 0.6483, 0.6951],\n",
      "        [0.6778, 0.6812, 0.9287]])\n",
      "add operation:\n",
      " tensor([[1.1094, 1.5774, 1.6273],\n",
      "        [1.5617, 1.5800, 1.9050]]) \n",
      " tensor([[1.1094, 1.5774, 1.6273],\n",
      "        [1.5617, 1.5800, 1.9050]]) \n",
      " tensor([[1.1094, 1.5774, 1.6273],\n",
      "        [1.5617, 1.5800, 1.9050]])\n",
      "Hadamard product:\n",
      " tensor([[0.1351, 1.4657, 1.5170],\n",
      "        [1.3804, 1.4202, 1.8598]]) \n",
      " tensor([[0.1351, 1.4657, 1.5170],\n",
      "        [1.3804, 1.4202, 1.8598]]) \n",
      " tensor([[0.1351, 1.4657, 1.5170],\n",
      "        [1.3804, 1.4202, 1.8598]])\n",
      "dot product:\n",
      " tensor([[2.7925, 3.2215],\n",
      "        [2.9179, 4.3124]]) \n",
      " tensor([[2.7925, 3.2215],\n",
      "        [2.9179, 4.3124]]) \n",
      " tensor([[2.7925, 3.2215],\n",
      "        [2.9179, 4.3124]])\n",
      "norm:\n",
      " tensor(2.0711) \n",
      " tensor(2.0711)\n",
      "index operation:\n",
      " tensor([0.9291, 0.8989]) \n",
      " tensor([0.1218, 0.9291, 0.9322])\n",
      "dimension operation:\n",
      " tensor([[0.1218, 0.9291],\n",
      "        [0.9322, 0.8839],\n",
      "        [0.8989, 0.9763]]) \n",
      " tensor([[0.1218],\n",
      "        [0.9291],\n",
      "        [0.9322],\n",
      "        [0.8839],\n",
      "        [0.8989],\n",
      "        [0.9763]]) \n",
      " tensor([[0.1218, 0.9291, 0.9322, 0.8839, 0.8989, 0.9763]]) \n",
      " tensor([0.1218, 0.9291, 0.9322, 0.8839, 0.8989, 0.9763])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(2, 3)\n",
    "y = torch.rand(2, 3)\n",
    "\n",
    "print(\"x, y:\\n\", x, \"\\n\", y)\n",
    "\n",
    "print(\"add operation:\\n\", x+y, \"\\n\", torch.add(x, y), \"\\n\", y.add_(x))\n",
    "\n",
    "print(\"Hadamard product:\\n\", x*y, \"\\n\", torch.mul(x, y), \"\\n\", y.mul_(x))\n",
    "\n",
    "print(\"dot product:\\n\", x@y.t(), \"\\n\", torch.mm(x, y.t()), \"\\n\", torch.matmul(x, y.t()))\n",
    "\n",
    "print(\"norm:\\n\", x.norm(2), \"\\n\", torch.norm(x, 2))\n",
    "\n",
    "print(\"index operation:\\n\", x[:, 1], \"\\n\", x[0, :])\n",
    "\n",
    "# torch.view()返回的新tensor与源tensor共享内存，即更改其中一个，另一个也会跟着改变\n",
    "print(\"dimension operation:\\n\", x.view(3, 2), \"\\n\", x.view(6, 1), \"\\n\", x.view(1, 6), \"\\n\", x.view(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[1, 2]])\n",
      "y:\n",
      " tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "x+y:\n",
      " tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(\"x:\\n\", x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(\"y:\\n\", y)\n",
    "print(\"x+y:\\n\", x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量/模型的保存与读写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2, y2:\n",
      " tensor([0, 1, 2, 3]) tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.arange(4)\n",
    "y = torch.zeros(4)\n",
    "torch.save([x, y], \"tensor.pt\")\n",
    "x2, y2 = torch.load(\"tensor.pt\")\n",
    "print(\"x2, y2:\\n\", x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_dict_:\n",
      " {'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "my_dict = {\"x\": x, \"y\": y}\n",
    "torch.save(my_dict, \"tensor_dict.pt\")\n",
    "my_dict_ = torch.load(\"tensor_dict.pt\")\n",
    "print(\"my_dict_:\\n\", my_dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y:\n",
      " tensor([[-0.3254, -0.2811, -0.2944, -0.0904,  0.0741,  0.0741,  0.2947, -0.0412,\n",
      "         -0.4655, -0.4603],\n",
      "        [ 0.1803,  0.2825,  0.2915, -0.3536,  0.2567,  0.0936,  0.0678,  0.1204,\n",
      "         -0.0874, -0.0599]], grad_fn=<AddmmBackward0>)\n",
      "clone:\n",
      " tensor([[True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.output(self.act(self.hidden(x)))\n",
    "    \n",
    "net = MLP()\n",
    "X = torch.randn(2, 20)\n",
    "Y = net(X)\n",
    "print(\"Y:\\n\", Y)\n",
    "\n",
    "torch.save(net.state_dict(), \"mlp.params\")\n",
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load(\"mlp.params\"))\n",
    "print(\"clone:\\n\", clone.eval()(X) == Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自动求导(autograd)\n",
    "`torch.Tensor`是autograd核心, 如果设置属性`.requires_grad_`为`True`则会追踪对于该张量的所有操作; 完成计算后可以通过调用`.backward()`, 来自动计算所有的梯度, 该张量的所有梯度会自动累加到`.grad`属性上, 因此在每次运行反向传播之前要将梯度清零\n",
    "\n",
    "1. 举例对函数$y=2x^Tx$关于列向量$x$求导\n",
    "2. 分离计算\n",
    "3. 非标量求梯度时要转为标量, 不可以对非标量进行`backward`。可以使用求和来求梯度, 因为求和并不影响梯度的求解结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([0., 1., 2., 3.])\n",
      "x.grad:\n",
      " None\n",
      "y:\n",
      " tensor(28., grad_fn=<MulBackward0>)\n",
      "x.grad:\n",
      " tensor([ 0.,  4.,  8., 12.])\n",
      "test grad:\n",
      " tensor([True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(4.0)\n",
    "print(\"x:\\n\", x)\n",
    "x.requires_grad_(True)\n",
    "print(\"x.grad:\\n\", x.grad)\n",
    "y = 2*torch.dot(x, x)\n",
    "print(\"y:\\n\", y)\n",
    "y.backward()\n",
    "print(\"x.grad:\\n\", x.grad)\n",
    "print(\"test grad:\\n\", x.grad == 4*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([0., 1., 2., 3.], requires_grad=True)\n",
      "test:\n",
      " tensor([True, True, True, True])\n",
      "x.grad:\n",
      " tensor([0., 2., 4., 6.])\n",
      "x.grad:\n",
      " tensor([0., 2., 4., 6.])\n",
      "test:\n",
      " tensor([True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(4.0, requires_grad=True)\n",
    "print(\"x:\\n\", x)\n",
    "y = x * x\n",
    "u = y.detach() # u不需要计算梯度, 视作常数\n",
    "z = u * x\n",
    "z.sum().backward()\n",
    "print(\"test:\\n\", x.grad == u)\n",
    "\n",
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "# 等价写法: y.backward(torch.ones(len(x)))\n",
    "print(\"x.grad:\\n\", x.grad)\n",
    "print(\"test:\\n\", x.grad == 2 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度学习基本配置\n",
    "* 常用包\n",
    "* 常用超参数\n",
    "    * batch\n",
    "    * learning rate\n",
    "    * max epochs\n",
    "* GPU配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' # 指明调用的GPU为0,1号\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据读入\n",
    "主要是通过`Dataset+DataLoader`方式完成, `Dataset`定义好数据的格式和数据变换形式, `DataLoader`用iterative的方式不断读入批次数据\n",
    "定义自己的`Dataset`主要有三个方法:\n",
    "* `__init__`: 向类中传入外部参数，同时定义样本集\n",
    "* `__getitem__`: 逐个读取样本集合中的元素，可以进行一定的变换，并将返回训练/验证所需的数据\n",
    "* `__len__`: 返回数据集的样本数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "# 自定义Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotations_file(string): Path to the csv file with annotations.\n",
    "            img_dir(string): Directory with all the images.\n",
    "            transform(callable, optional): Optional transform to be applied on a sample.\n",
    "            target_transform(callable, optional): Optional transform to be applied on the target.\n",
    "        \"\"\"\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx(int): Index\n",
    "        \"\"\"\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "data_transform = \"\"\n",
    "train_data = datasets.ImageFolder(train_path=\"\", transform=data_transform)\n",
    "val_data = datasets.ImageFolder(val_path=\"\", transform=data_transform)\n",
    "# 使用DataLoader加载数据\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=4, shuffle=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型构建\n",
    "* Pytorch中神经网络一般基于`torch.nn.Module`类的模型, 只需要定义`forward`函数, `backward`函数会在使用`autograd`时自动定义, `backward`函数用来计算导数\n",
    "* 神经网路中常见的层\n",
    "    * 不含模型参数的层\n",
    "    * 含模型参数的层\n",
    "    * 二维卷积层\n",
    "    * 池化层\n",
    "* 神经网络的典型训练过程\n",
    "    1. 定义包含一些可学习参数(或者叫权重)的神经网络\n",
    "    2. 在输入数据集上迭代\n",
    "    3. 通过网络处理输入\n",
    "    4. 计算loss(输出和正确答案的距离)\n",
    "    5. 将梯度反向传播给网络的参数\n",
    "    6. 更新网络的权重, 一般使用一个简单的规则: `weight = weight - learning_rate * gradient`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (activate): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "tensor([[-0.2646,  0.1430, -0.0876, -0.2220,  0.0971,  0.1070, -0.0418,  0.1346,\n",
      "          0.0478,  0.2612],\n",
      "        [-0.3111,  0.1876, -0.1022, -0.1682,  0.0499, -0.0197, -0.0717,  0.1531,\n",
      "          0.0171,  0.2583]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  # 声明带有模型参数的层, 这里声明了两个全连接层\n",
    "  def __init__(self, **kwargs):\n",
    "    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数\n",
    "    super(MLP, self).__init__(**kwargs)\n",
    "    self.hidden = nn.Linear(784, 256)\n",
    "    self.activate = nn.ReLU()\n",
    "    self.output = nn.Linear(256,10)\n",
    "    \n",
    "   # 定义模型的前向计算, 即如何根据输入x计算返回所需要的模型输出\n",
    "   # 无须定义反向传播函数。系统将通过自动求梯度而自动生成反向传播所需的backward函数\n",
    "  def forward(self, x):\n",
    "    out = self.activate(self.hidden(x))\n",
    "    return self.output(out)  \n",
    "\n",
    "X = torch.rand(2, 784)\n",
    "net = MLP()\n",
    "print(net)\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2., -1.,  0.,  1.,  2.])\n"
     ]
    }
   ],
   "source": [
    "# 不含模型参数的自定义层\n",
    "import torch\n",
    "from torch import nn   \n",
    "class MyLayer(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "    def forward(self, x):\n",
    "        return x - x.mean()\n",
    "    \n",
    "layer = MyLayer()\n",
    "print(layer(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyListDense(\n",
      "  (params): ParameterList(\n",
      "      (0): Parameter containing: [torch.float32 of size 4x4]\n",
      "      (1): Parameter containing: [torch.float32 of size 4x4]\n",
      "      (2): Parameter containing: [torch.float32 of size 4x4]\n",
      "      (3): Parameter containing: [torch.float32 of size 4x1]\n",
      "  )\n",
      ")\n",
      "MyDictDense(\n",
      "  (params): ParameterDict(\n",
      "      (linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (linear2): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "      (linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 含模型参数的自定义层\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MyListDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyListDense, self).__init__()\n",
    "        # Parameter是Tensor的子类, 其会自动被添加到模型的参数列表里\n",
    "        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])\n",
    "        self.params.append(nn.Parameter(torch.randn(4, 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.params)):\n",
    "            x = torch.mm(x, self.params[i])\n",
    "        return x\n",
    "net = MyListDense()\n",
    "print(net)\n",
    "\n",
    "class MyDictDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyDictDense, self).__init__()\n",
    "        self.params = nn.ParameterDict({\n",
    "                'linear1': nn.Parameter(torch.randn(4, 4)),\n",
    "                'linear2': nn.Parameter(torch.randn(4, 1))\n",
    "        })\n",
    "        self.params.update({'linear3': nn.Parameter(torch.randn(4, 2))}) # 新增\n",
    "\n",
    "    def forward(self, x, choice='linear1'):\n",
    "        return torch.mm(x, self.params[choice])\n",
    "\n",
    "net = MyDictDense()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二维卷积层, 将输入和卷积核做互相关运算, 并加上一个标量偏差来得到输出\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 卷积运算（二维互相关）\n",
    "def corr2d(X, K): \n",
    "    h, w = K.shape\n",
    "    X, K = X.float(), K.float()\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
    "    return Y\n",
    "\n",
    "# 二维卷积层, 卷积层的模型参数包括了卷积核和标量偏差\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Conv2D, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 池化层, 每次对输入数据的一个固定形状窗口中的元素计算输出\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 前向计算的实现\n",
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
    "    return Y\n",
    "\n",
    "X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]], dtype=torch.float)\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型初始化\n",
    "`torch.nn.init`提供以下初始化方式:\n",
    "1. `torch.nn.init.uniform_(tensor, a=0.0, b=1.0)`\n",
    "2. `torch.nn.init.normal_(tensor, mean=0.0, std=1.0)`\n",
    "3. `torch.nn.init.constant_(tensor, val)`\n",
    "4. `torch.nn.init.ones_(tensor)`\n",
    "5. `torch.nn.init.zeros_(tensor)`\n",
    "6. `torch.nn.init.eye_(tensor)`\n",
    "7. `torch.nn.init.dirac_(tensor, groups=1)`\n",
    "8. `torch.nn.init.xavier_uniform_(tensor, gain=1.0)`\n",
    "9. `torch.nn.init.xavier_normal_(tensor, gain=1.0)`\n",
    "10. `torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan__in', nonlinearity='leaky_relu')`\n",
    "11. `torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')`\n",
    "12. `torch.nn.init.orthogonal_(tensor, gain=1)`\n",
    "13. `torch.nn.init.sparse_(tensor, sparsity, std=0.01)`\n",
    "\n",
    "以上函数均直接原地更改输入张量的值\n",
    "\n",
    "14. `torch.nn.init.calculate_gain(nonlinearity, param=None)`\n",
    "\n",
    "<img src=\"pic/pic2.png\" width = \"50%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test type:\n",
      " True\n",
      "weight:\n",
      " tensor([[[[ 0.2422,  0.1846,  0.1458],\n",
      "          [-0.1550, -0.2580, -0.1129],\n",
      "          [ 0.3068,  0.2992, -0.0534]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2793, -0.1693, -0.1054],\n",
      "          [ 0.0284, -0.1615, -0.1221],\n",
      "          [ 0.1241, -0.2343,  0.2210]]],\n",
      "\n",
      "\n",
      "        [[[-0.2649,  0.0559,  0.1013],\n",
      "          [-0.0170,  0.0444,  0.1168],\n",
      "          [-0.0975,  0.1835,  0.0817]]]])\n",
      "weight:\n",
      " tensor([[[[ 0.3414,  0.9009,  0.2158],\n",
      "          [ 0.0270,  0.3259, -0.1581],\n",
      "          [-0.4717,  0.1819, -0.0927]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9349,  0.0406, -0.0252],\n",
      "          [ 0.2589,  0.6081,  0.5480],\n",
      "          [-0.1269, -0.1519,  0.9163]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1861,  0.2320,  0.5892],\n",
      "          [ 0.2482, -0.3617, -0.0351],\n",
      "          [-0.0309, -0.0220, -0.2693]]]])\n",
      "\n",
      "-------MLP-------\n",
      "\n",
      "tensor([[[[ 0.1814,  0.2181, -0.0305],\n",
      "          [-0.3044,  0.1807,  0.1593],\n",
      "          [ 0.2541, -0.2394,  0.1537]]]])\n",
      "\n",
      "-------初始化-------\n",
      "\n",
      "tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "conv = nn.Conv2d(1, 3, 3)\n",
    "print(\"test type:\\n\", isinstance(conv, nn.Conv2d))\n",
    "print(\"weight:\\n\", conv.weight.data)\n",
    "torch.nn.init.kaiming_normal_(conv.weight.data)\n",
    "print(\"weight:\\n\", conv.weight.data)\n",
    "\n",
    "# e.g.\n",
    "# 封装初始化函数\n",
    "def initialize_weights(model):\n",
    "\tfor m in model.modules():\n",
    "\t\t# 判断是否属于Conv2d\n",
    "\t\tif isinstance(m, nn.Conv2d):\n",
    "\t\t\ttorch.nn.init.zeros_(m.weight.data)\n",
    "\t\t\t# 判断是否有偏置\n",
    "\t\t\tif m.bias is not None:\n",
    "\t\t\t\ttorch.nn.init.constant_(m.bias.data,0.3)\n",
    "\t\telif isinstance(m, nn.Linear):\n",
    "\t\t\ttorch.nn.init.normal_(m.weight.data, 0.1)\n",
    "\t\t\tif m.bias is not None:\n",
    "\t\t\t\ttorch.nn.init.zeros_(m.bias.data)\n",
    "\t\telif isinstance(m, nn.BatchNorm2d):\n",
    "\t\t\tm.weight.data.fill_(1) \t\t \n",
    "\t\t\tm.bias.data.zeros_()\t\n",
    "\n",
    "print(\"\\n-------MLP-------\\n\")\n",
    "# 模型的定义\n",
    "class MLP(nn.Module):\n",
    "  # 声明带有模型参数的层，这里声明了两个全连接层\n",
    "  def __init__(self, **kwargs):\n",
    "    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数\n",
    "    super(MLP, self).__init__(**kwargs)\n",
    "    self.hidden = nn.Conv2d(1,1,3)\n",
    "    self.act = nn.ReLU()\n",
    "    self.output = nn.Linear(10,1)\n",
    "    \n",
    "   # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出\n",
    "  def forward(self, x):\n",
    "    o = self.act(self.hidden(x))\n",
    "    return self.output(o)\n",
    "\n",
    "mlp = MLP()\n",
    "print(mlp.hidden.weight.data)\n",
    "print(\"\\n-------初始化-------\\n\")\n",
    "\n",
    "mlp.apply(initialize_weights)\n",
    "# 或者initialize_weights(mlp)\n",
    "print(mlp.hidden.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数\n",
    "1. 二分类交叉熵损失函数`torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')`\n",
    "    * `weight`: 每个类别的`loss`设置权值\n",
    "    * `size_average`: 设置为`True`返回`loss`均值, 否则返回各样本`loss`之和\n",
    "    * `reduce`: 设置为`True`返回标量\n",
    "2. 交叉熵损失函数`torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')`\n",
    "    * `ignore_index`: 忽略某个类的损失函数\n",
    "\n",
    "    <img src=\"pic/pic3.png\" width = \"75%\" />\n",
    "\n",
    "3. `L1`损失函数`torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')`, $l_n=|x_n-y_n|$\n",
    "    * `reduction`决定了计算模式, 设置为`none`则逐个元素计算; 设置为`sum`则所有元素求和; 设置为`mean`则加权平均返回标量\n",
    "4. `MSE`损失函数`torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')`, $l_n=(x_n-y_n)^2$\n",
    "5. 平滑`L1(Smooth L1)`损失函数`torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean', beta=1.0)`\n",
    "    \n",
    "    <img src=\"pic/pic4.png\" width = \"75%\" />\n",
    "\n",
    "6. 目标泊松分布的负对数似然损失`torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')`\n",
    "    * `log_input`: 输入是否为对数形式\n",
    "    * `full`: 计算所有的损失\n",
    "    * `eps`: 修正项，避免`input`为0时, `log(input)`为nan的情况\n",
    "\n",
    "    <img src=\"pic/pic5.png\" width = \"65%\" />\n",
    "\n",
    "7. `KL`散度`torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean', log_target=False)`\n",
    "\n",
    "    <img src=\"pic/pic6.png\" width = \"65%\" />\n",
    "\n",
    "8. `MarginRankingLoss`损失函数`torch.nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')`, $l(x_1,x_2,y)=\\max(0,-y\\times(x_1-x_2)+margin)$\n",
    "    * `margin`: 边界值, $x_1$和$x_2$之间的差异值\n",
    "9. 多标签边界损失函数`torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean')`\n",
    "\n",
    "    <img src=\"pic/pic7.png\" width = \"80%\" />\n",
    "\n",
    "10. 二分类损失函数`torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction='mean')`\n",
    "\n",
    "    <img src=\"pic/pic8.png\" width = \"80%\" />\n",
    "\n",
    "11. 多分类折页损失`torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean')`\n",
    "\n",
    "    <img src=\"pic/pic9.png\" width = \"80%\" />\n",
    "\n",
    "12. 三元组损失`torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')`\n",
    "    * 三元组: <实体1, 关系, 实体2>。在项目中, 也可以表示为`<anchor, positive examples , negative examples>`, 希望`anchor`的距离更接近`positive examples`更远离`negative examples`\n",
    "\n",
    "    <img src=\"pic/pic10.png\" width = \"65%\" />\n",
    "\n",
    "13. `HingEmbeddingLoss`损失函数`torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean')`\n",
    "\n",
    "    <img src=\"pic/pic11.png\" width = \"80%\" />    \n",
    "\n",
    "14. 余弦相似度`torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')`\n",
    "15. `CTC`损失函数`torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCELoss损失函数的计算结果为 tensor(0.7857, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "output = loss(m(input), target)\n",
    "output.backward()\n",
    "print('BCELoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss损失函数的计算结果为 tensor(1.4290, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('CrossEntropyLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1Loss损失函数的计算结果为 tensor(1.0457, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "loss = nn.L1Loss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('L1Loss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSELoss损失函数的计算结果为 tensor(1.7451, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('MSELoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmoothL1Loss损失函数的计算结果为 tensor(0.7535, grad_fn=<SmoothL1LossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "loss = nn.SmoothL1Loss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('SmoothL1Loss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoissonNLLLoss损失函数的计算结果为 tensor(1.3255, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "loss = nn.PoissonNLLLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('PoissonNLLLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLDivLoss损失函数的计算结果为 tensor(-0.3335)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "# 不需要反向传播\n",
    "loss = nn.KLDivLoss()\n",
    "input = torch.tensor([[0.5, 0.3, 0.2], [0.2, 0.3, 0.5]])\n",
    "target = torch.tensor([[0.9, 0.05, 0.05], [0.1, 0.7, 0.2]], dtype=torch.float)\n",
    "output = loss(input, target)\n",
    "print('KLDivLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MarginRankingLoss损失函数的计算结果为 tensor(0.8786, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "loss = nn.MarginRankingLoss()\n",
    "input1 = torch.randn(3, requires_grad=True)\n",
    "input2 = torch.randn(3, requires_grad=True)\n",
    "target = torch.randn(3).sign()\n",
    "output = loss(input1, input2, target)\n",
    "output.backward()\n",
    "print('MarginRankingLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLabelMarginLoss损失函数的计算结果为 tensor(0.4500)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "loss = nn.MultiLabelMarginLoss()\n",
    "x = torch.FloatTensor([[0.9, 0.2, 0.4, 0.8]])\n",
    "# for target y, only consider labels 3 and 0, not after label -1\n",
    "y = torch.LongTensor([[3, 0, -1, 1]])# 真实的分类是，第3类和第0类\n",
    "output = loss(x, y)\n",
    "\n",
    "print('MultiLabelMarginLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftMarginLoss损失函数的计算结果为 tensor(0.6037)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "loss = nn.SoftMarginLoss()\n",
    "input = torch.tensor([[0.3, 0.7], [0.5, 0.5]])\n",
    "target = torch.tensor([[1.0, 0.0], [0.0, 1.0]], dtype=torch.float)\n",
    "output = loss(input, target)\n",
    "print('SoftMarginLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiMarginLoss损失函数的计算结果为 tensor(0.4000)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "loss = nn.MultiMarginLoss()\n",
    "input = torch.tensor([[0.3, 0.7], [0.5, 0.5]])\n",
    "target = torch.tensor([1, 0], dtype=torch.long)\n",
    "output = loss(input, target)\n",
    "print('MultiMarginLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TripletMarginLoss损失函数的计算结果为 tensor(1.0236, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "anchor = torch.randn(100, 128, requires_grad=True)\n",
    "positive = torch.randn(100, 128, requires_grad=True)\n",
    "negative = torch.randn(100, 128, requires_grad=True)\n",
    "output = loss(anchor, positive, negative)\n",
    "output.backward()\n",
    "print('TripletMarginLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HingeEmbeddingLoss损失函数的计算结果为 tensor(0.7667)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "loss = nn.HingeEmbeddingLoss()\n",
    "input = torch.tensor([[1., 0.8, 0.5]])\n",
    "target = torch.tensor([[1, 1, -1]])\n",
    "output = loss(input, target)\n",
    "print('HingeEmbeddingLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CosineEmbeddingLoss损失函数的计算结果为 tensor(0.5096, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "loss = nn.CosineEmbeddingLoss()\n",
    "input1 = torch.randn(100, 128, requires_grad=True)\n",
    "input2 = torch.randn(100, 128, requires_grad=True)\n",
    "target = torch.randn(100).sign()\n",
    "output = loss(input1, input2, target)\n",
    "output.backward()\n",
    "print('CosineEmbeddingLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model的训练验证\n",
    "#### 训练\n",
    "```python\n",
    "def train(epoch):\n",
    "    # 训练状态\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    # 用for循环读取DataLoader中的全部数据\n",
    "    for data, label in train_loader:\n",
    "        data, label = data.cuda(), label.cuda()\n",
    "        # 将优化器梯度置为0\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\t\tprint('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
    "```\n",
    "#### 验证\n",
    "```python\n",
    "def val(epoch):       \n",
    "    # 验证状态\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    # 验证时梯度保持不变\n",
    "    with torch.no_grad():\n",
    "        for data, label in val_loader:\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "            output = model(data)\n",
    "            preds = torch.argmax(output, 1)\n",
    "            loss = criterion(output, label)\n",
    "            val_loss += loss.item()*data.size(0)\n",
    "            running_accu += torch.sum(preds == label.data)\n",
    "    val_loss = val_loss/len(val_loader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, val_loss))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch优化器\n",
    "\n",
    "<img src=\"pic/pic12.png\" width = \"75%\" />\n",
    "\n",
    "`Optimizer`的三个属性 \n",
    "* `defaults`: 存储优化器的超参数(是一个dict)\n",
    "* `state`: 参数的缓存\n",
    "* `param_groups`: 管理的参数组(是一个list, 其中每个元素是一个dict)\n",
    "\n",
    "`Optimizer`包括的几个方法\n",
    "* `zero_grad()`: 清空所管理参数的梯度\n",
    "* `step()`: 执行一步梯度更新(参数更新)\n",
    "* `add_param_group()`: 添加参数组\n",
    "* `load_state_dict()`: 加载状态参数字典(常用于模型的断点续训练, 继续上次的参数进行训练)\n",
    "* `state_dict()`: 获取优化器当前状态信息字典\n",
    "\n",
    "`Optimizer`在一个`epoch`中:\n",
    "```python\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-5)\n",
    "for epoch in range(EPOCH):\n",
    "\t...\n",
    "\toptimizer.zero_grad()  #梯度置零\n",
    "\tloss = ...             #计算loss\n",
    "\tloss.backward()        #BP反向传播\n",
    "\toptimizer.step()       #梯度更新\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
